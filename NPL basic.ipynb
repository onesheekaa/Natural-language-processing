{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piFwYfl5zcW0"
   },
   "source": [
    "1. Obtain the Named entity relations in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tis966MZlrds",
    "outputId": "29a74574-0db7-42f5-835a-4ab924264701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n",
      "Bill Gates PERSON\n",
      "Paul Allen PERSON\n",
      "Redmond GPE\n",
      "Washington GPE\n",
      "[('Microsoft', 'Bill Gates'), ('Microsoft', 'Paul Allen'), ('Microsoft', 'Redmond'), ('Microsoft', 'Washington'), ('Bill Gates', 'Microsoft'), ('Bill Gates', 'Paul Allen'), ('Bill Gates', 'Redmond'), ('Bill Gates', 'Washington'), ('Paul Allen', 'Microsoft'), ('Paul Allen', 'Bill Gates'), ('Paul Allen', 'Redmond'), ('Paul Allen', 'Washington'), ('Redmond', 'Microsoft'), ('Redmond', 'Bill Gates'), ('Redmond', 'Paul Allen'), ('Redmond', 'Washington'), ('Washington', 'Microsoft'), ('Washington', 'Bill Gates'), ('Washington', 'Paul Allen'), ('Washington', 'Redmond')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"Microsoft, founded by Bill Gates and Paul Allen, is headquartered in Redmond, Washington.\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Named Entity Recognition\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# To extract relations, one basic method is to look for patterns or dependencies between entities\n",
    "relations = []\n",
    "for ent1 in doc.ents:\n",
    "    for ent2 in doc.ents:\n",
    "        if ent1 != ent2:  # Ensure they are not the same entity\n",
    "            # This is a very simplistic way to establish a relation, real-world usage requires more complex logic\n",
    "            relations.append((ent1.text, ent2.text))\n",
    "\n",
    "print(relations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyoGTwewWCZd"
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPw6JvtOWDp6",
    "outputId": "3140fc5e-11a6-470a-a07c-038c03b133c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. In addition to its financial results, Apple announced several new initiatives during the earnings call, including plans to expand its renewable energy projects and investments in workforce development programs.\n",
      "2. \"We're thrilled with our results for the quarter, which set new records for revenue, earnings, and iPhone sales,\" said Cook during the earnings call with analysts.\n",
      "3. \n",
      "Apple Inc. reported strong quarterly results on Tuesday, bolstered by strong sales of iPhones, which the company said were up 17% from a year earlier.\n",
      "4. Analysts cited concerns about supply chain constraints and the impact of geopolitical tensions on the company's business in China as potential reasons for the dip.\n",
      "5. Apple's services segment, which includes revenue from the App Store, iCloud, and Apple Music, also saw significant growth, with revenue increasing by 15% year-over-year.\n",
      "6. CEO Tim Cook attributed the strong performance to robust demand for the iPhone 13 lineup and continued growth in the company's services segment.\n",
      "7. The company said it expects continued strong demand for its products and services, despite ongoing challenges related to the global supply chain.\n",
      "8. The company also highlighted its commitment to privacy and security, emphasizing the importance of protecting user data in the digital age.\n",
      "9. \"Our customers continue to love the iPhone 13 lineup, and we're seeing strong demand across our product categories.\"\n",
      "10. The tech giant posted revenue of $123.9 billion for the quarter ending December 31, beating analyst expectations.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample news article text\n",
    "news_article_text = \"\"\"\n",
    "Apple Inc. reported strong quarterly results on Tuesday, bolstered by strong sales of iPhones, which the company said were up 17% from a year earlier. The tech giant posted revenue of $123.9 billion for the quarter ending December 31, beating analyst expectations. CEO Tim Cook attributed the strong performance to robust demand for the iPhone 13 lineup and continued growth in the company's services segment.\n",
    "\n",
    "\"We're thrilled with our results for the quarter, which set new records for revenue, earnings, and iPhone sales,\" said Cook during the earnings call with analysts. \"Our customers continue to love the iPhone 13 lineup, and we're seeing strong demand across our product categories.\"\n",
    "\n",
    "Apple's services segment, which includes revenue from the App Store, iCloud, and Apple Music, also saw significant growth, with revenue increasing by 15% year-over-year. The company reported double-digit growth in each of its geographic segments, with particularly strong performance in China.\n",
    "\n",
    "Despite the positive results, Apple's stock fell slightly in after-hours trading following the earnings announcement. Analysts cited concerns about supply chain constraints and the impact of geopolitical tensions on the company's business in China as potential reasons for the dip.\n",
    "\n",
    "Looking ahead, Apple provided guidance for the current quarter, forecasting revenue between $107 billion and $110 billion. The company said it expects continued strong demand for its products and services, despite ongoing challenges related to the global supply chain.\n",
    "\n",
    "In addition to its financial results, Apple announced several new initiatives during the earnings call, including plans to expand its renewable energy projects and investments in workforce development programs. The company also highlighted its commitment to privacy and security, emphasizing the importance of protecting user data in the digital age.\n",
    "\n",
    "Overall, Apple's strong performance in the latest quarter reflects the company's resilience amid challenging market conditions. With continued innovation and strategic investments, Apple remains well-positioned for future growth and success.\n",
    "\"\"\"\n",
    "\n",
    "def get_top_sentences(news_text, top_n=10):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(news_text)\n",
    "\n",
    "    # Calculate the length of each sentence\n",
    "    sentence_lengths = {sentence: len(sentence.split()) for sentence in sentences}\n",
    "\n",
    "    # Rank sentences based on their lengths\n",
    "    ranked_sentences = sorted(sentence_lengths.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N sentences\n",
    "    top_sentences = [sentence[0] for sentence in ranked_sentences[:top_n]]\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "# Get the top sentences from the news article text\n",
    "top_sentences = get_top_sentences(news_article_text)\n",
    "\n",
    "# Output the top sentences\n",
    "for i, sentence in enumerate(top_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwK3G54DUEcP"
   },
   "source": [
    "3. Implement a text classification application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0AveW89UIJ7",
    "outputId": "f4684189-b506-4294-d3c9-82b724b41ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9680851063829787\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.98      0.95      0.97       175\n",
      "         comp.graphics       0.95      1.00      0.98       200\n",
      "               sci.med       0.97      0.96      0.96       200\n",
      "soc.religion.christian       0.97      0.95      0.96       177\n",
      "\n",
      "              accuracy                           0.97       752\n",
      "             macro avg       0.97      0.97      0.97       752\n",
      "          weighted avg       0.97      0.97      0.97       752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data into numerical feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "classifier = SVC(kernel='linear', random_state=42)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels of test data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgPm_Wm5V1yB"
   },
   "source": [
    "4 . Implement a text clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjoKr0n8VzkP",
    "outputId": "2c61d63f-ba03-4040-a2fa-5897d5f7f485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers:\n",
      "Cluster 0:\n",
      " document\n",
      " second\n",
      "\n",
      "Cluster 1:\n",
      " second\n",
      " document\n",
      "\n",
      "New Sample belongs to Cluster: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Convert text data to numerical vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Clustering\n",
    "true_k = 2  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=true_k)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print cluster centers and associated words\n",
    "print(\"Cluster centers:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print()\n",
    "\n",
    "# Predicting the cluster for new samples\n",
    "new_samples = [\"This is a new document.\"]\n",
    "X_new = vectorizer.transform(new_samples)\n",
    "predicted = kmeans.predict(X_new)\n",
    "print(\"New Sample belongs to Cluster:\", predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxCcuZGPUbA-"
   },
   "source": [
    "5 . Implement a web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RglmCnVHUYEk",
    "outputId": "92c0a996-bb7a-49da-9073-a5f9dc5c930e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://example.com\n",
      "Crawling: https://www.iana.org/domains/example\n",
      "Crawling: https://www.icann.org/privacy/tos\n",
      "Crawling: http://www.icann.org/\n",
      "Crawling: http://pti.icann.org\n",
      "Error: HTTPConnectionPool(host='pti.icann.org', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7871cacecb20>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Crawling: https://www.icann.org/privacy/policy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def extract_links(url):\n",
    "    # Send an HTTP request to the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for non-200 status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags (links) in the HTML content\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Extract the href attribute from each anchor tag\n",
    "    extracted_links = [link.get('href') for link in links if link.get('href') is not None]\n",
    "\n",
    "    return extracted_links\n",
    "\n",
    "def crawl_web(start_url, depth):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = set([start_url])\n",
    "    current_depth = 0\n",
    "\n",
    "    while current_depth <= depth:\n",
    "        next_urls_to_visit = set()\n",
    "\n",
    "        for url in urls_to_visit:\n",
    "            if url not in visited_urls:\n",
    "                print(\"Crawling:\", url)\n",
    "                visited_urls.add(url)\n",
    "\n",
    "                # Extract links from the current URL\n",
    "                links = extract_links(url)\n",
    "\n",
    "                # Add new links to the set of URLs to visit next\n",
    "                for link in links:\n",
    "                    if link is not None and link.startswith('http'):\n",
    "                        next_urls_to_visit.add(link)\n",
    "\n",
    "        # Move to the next depth level\n",
    "        urls_to_visit = next_urls_to_visit\n",
    "        current_depth += 1\n",
    "\n",
    "        # Add a delay of 1 second between requests to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "\n",
    "# Example usage\n",
    "start_url = 'https://example.com'  # Starting URL\n",
    "depth = 2  # Maximum depth to crawl\n",
    "crawl_web(start_url, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC9NT5RPVjNx"
   },
   "source": [
    "6. Write an application for social media mining?\n",
    "\n",
    "\n",
    "Creating a social media mining application involves collecting, processing, and analyzing data from various social media platforms. Here's a basic outline for building such an application:\n",
    "\n",
    "Select Social Media Platforms: Choose the social media platforms from which you want to gather data. Examples include Twitter, Facebook, Instagram, LinkedIn, Reddit, etc.\n",
    "\n",
    "Set Up API Access: Register for developer accounts and obtain API access tokens for the selected social media platforms. APIs allow you to programmatically access data from these platforms.\n",
    "\n",
    "Define Data Sources: Determine the specific types of data you want to collect, such as posts, comments, likes, followers, etc. Define search queries or filters to retrieve relevant data.\n",
    "\n",
    "Collect Data: Use the APIs provided by social media platforms to collect data based on your defined sources and criteria. Store the collected data in a database or file system for further processing.\n",
    "\n",
    "Preprocess Data: Clean and preprocess the collected data by removing noise, filtering out irrelevant information, and transforming it into a format suitable for analysis.\n",
    "\n",
    "Analyze Data: Apply various data mining and analysis techniques to gain insights from the collected data. This may include sentiment analysis, topic modeling, trend detection, network analysis, etc.\n",
    "\n",
    "Visualize Results: Visualize the analyzed data using charts, graphs, word clouds, network diagrams, etc., to make the insights more understandable and actionable.\n",
    "\n",
    "Implement Features: Depending on your application's objectives, implement features such as real-time monitoring, keyword tracking, user profiling, etc., to provide valuable functionalities to users.\n",
    "\n",
    "Deploy the Application: Deploy your social media mining application on a server or cloud platform so that users can access and use it. Ensure scalability, reliability, and security of the deployed application.\n",
    "\n",
    "Monitor and Maintain: Regularly monitor the performance of your application, address any issues or bugs that arise, and update the application with new features or improvements as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlQ0shuWWjF8"
   },
   "source": [
    "7. Desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC79iPb1WPUg",
    "outputId": "84defea0-e56e-4a3b-85b6-3630913e1968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['No']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert categorical variables to numerical labels\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop('PlayTennis', axis=1)\n",
    "y = df['PlayTennis']\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Define new instance to predict\n",
    "new_data = {\n",
    "    'Outlook': ['Sunny'],\n",
    "    'Temperature': ['Hot'],\n",
    "    'Humidity': ['High'],\n",
    "    'Wind': ['Weak']\n",
    "}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Transform new instance with label encoders\n",
    "for column in new_df.columns:\n",
    "    new_df[column] = label_encoders[column].transform(new_df[column])\n",
    "\n",
    "# Predict\n",
    "prediction = clf.predict(new_df)\n",
    "print(\"Prediction:\", label_encoders['PlayTennis'].inverse_transform(prediction))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
